[package]
name = "candle-flash-attn-v3"
version = "0.0.1-rc0"
edition = "2021"

description = "Flash attention V3 layer for the candle ML framework."
keywords = ["blas", "tensor", "machine-learning"]
categories = ["science"]
license = "MIT OR Apache-2.0"
readme = "README.md"
authors = ["Michael Feil"]
repository = "https://github.com/michaelfeil/candle-flash-attn-v3"

[lib]
name = "candle_flash_attn_v3"
path = "src/lib.rs"

[features]
default = ["cuda-12"]
# Use cuda-12 for candle 0.9+ with cudarc 0.16+ (new API)
# Use cuda-11 for older candle versions (pre-0.9) with older cudarc API
cuda-12 = []
cuda-11 = []

[dependencies]
candle = { version = ">=0.1", package = "candle-core", features = ["cuda"]}
half = { version = "2.3.1", features = ["num-traits"] }

[build-dependencies]
anyhow = { version = "1", features = ["backtrace"] }
num_cpus = "1.15.0"
rayon = "1.7.0"
tar = "0.4"
zstd = "0.13"


[dev-dependencies]
anyhow = { version = "1", features = ["backtrace"] }
candle-nn = { version = "0.9.1", features = ["cuda"] }
rstest = "0.23"